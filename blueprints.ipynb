{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ad10a6-3276-41a6-94d4-05364835497c",
   "metadata": {},
   "source": [
    "### visulaizing the  corine images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f9a31",
   "metadata": {},
   "source": [
    "### The corine land cover classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "760bd90b-70a6-4959-991c-7faa9d7b1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "clcc ={111:'cont. urban fabric',\n",
    " 112:'disc urban fabric',\n",
    " 121:'industrial or commercial units',\n",
    " 122:'road and rail',\n",
    " 123:'port areas',\n",
    " 124:'airports',\n",
    " 131:'mineral extraction sites',\n",
    " 132:'dump sites',\n",
    " 133:'construction sites',\n",
    " 141:'green urban areas',\n",
    " 142:'sport and leasure',\n",
    " 211:'non irregated arable land',\n",
    " 212:'permenant irregated land',\n",
    " 213:'rice fields',\n",
    " 221:'vine yards',\n",
    " 223:'olive groves',\n",
    " 231:'pastures',\n",
    " 241:'annual with perm. crops',\n",
    " 242:'complex cultivation patters',\n",
    " 243:'land principally occupied by agriculture',\n",
    " 244:'agro forest areas',\n",
    " 311:'broad leaved forest',\n",
    " 312:'conferous forest',\n",
    " 313:'mixed forest',\n",
    " 321:'natural grassland',\n",
    " 322:'moors and heathland',\n",
    " 323: 'scierohllous vegitation',\n",
    " 324:'transitional woodland shrub',\n",
    " 331: 'beaches dunes and sand plains',\n",
    " 332:'bare rock',\n",
    " 333:'sparsely vegetated areas',\n",
    " 334:'burnt areas',\n",
    " 335:'glaciers and perpetual snow',\n",
    " 411:'inland marshes',\n",
    " 412:'peat bogs',\n",
    " 421:'salt marshes',\n",
    " 422:'salines',\n",
    " 423:'intertidal flats',\n",
    " 511:'water courses',\n",
    " 512:'water bodies',\n",
    " 521:'costal lagoons',\n",
    " 522:'estuaries',\n",
    " 523:'sea and ocean'}\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e10dd5-ddb6-483b-b053-7d0bd6079b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "landcovers = [key for idx, key in enumerate(clcc) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64adb4e5-338e-4639-a3de-15bf16188e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(landcovers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b43d91",
   "metadata": {},
   "source": [
    "### Algorithm to change a segmentation mask into a feature vector \n",
    "#### where each element represnts the number of pixels of each class in each segmentation mask\n",
    "#### The first part is a dataloader for the segmentation mask and the labels of wild and non wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ee2631-df09-4829-93de-0e7e6490dc04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'landcovers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8708/3437187090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#print(images[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Iterate over the data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlandcovers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mfeature_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mtarget_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'landcovers' is not defined"
     ]
    }
   ],
   "source": [
    "#final code\n",
    "# to do list is to build a transparent model to train on the classes\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        self.image_paths = sorted(glob.glob(os.path.join(root)+ '/*.*'))\n",
    "        #self.image_paths = (self.data['file'])\n",
    "        #print(len(self.image_paths))\n",
    "        #self.image_paths = sorted(glob.glob(os.path.join('/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs', self.image_paths)))\n",
    "        self.labels = self.data['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "\n",
    "        \n",
    "        # we need to open the image as a tiff file and take the first channel representing the corine channel\n",
    "        image = tiff.imread(image_path)\n",
    "        image = image.astype('int')\n",
    "        #print(image.shape)\n",
    "        image = image[:,:,0]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file = 'infos.csv'\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "dataset = CustomDataset(csv_file,root = '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs')\n",
    "\n",
    "# Create a data loader to iterate over the dataset\n",
    "batch_size = 1\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# code the image to segmentation mask function:\n",
    "# to extract the corine channel from the path\n",
    "#print(len(data_loader))\n",
    "#print(images[0])\n",
    "# Iterate over the data loader\n",
    "array = landcovers\n",
    "feature_array = []\n",
    "target_labels = []\n",
    "for images, labels in tqdm(data_loader):\n",
    "    # Convert images to segmentation mask here using your own logic\n",
    "    #print('-------loop is working-------')\n",
    "    segmentation_mask = images\n",
    "    #print(len(images))\n",
    "    # Get the unique land cover classes present in the segmentation mask\n",
    "    land_cover_classes = torch.unique(segmentation_mask)\n",
    "\n",
    "    # Compute the number of land cover classes and the maximum number of classes expected\n",
    "    num_classes = len(land_cover_classes)\n",
    "    max_num_classes = 44  # Set the maximum number of land cover classes expected in the dataset\n",
    "    \n",
    "    \n",
    "    # create a dictionary to have the key the land cover class code and as the ni. of pixels of this specific land cover class as data\n",
    "    class_counts = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate the vector with the count of pixels for each land cover class\n",
    "    \n",
    "    for class_keys in land_cover_classes:\n",
    "        class_counts = {key: 0 for key in array}\n",
    "\n",
    "        if class_keys in landcovers: # to make sure that we only have the corine land cover classess\n",
    "            \n",
    "\n",
    "        \n",
    "            class_rep = torch.sum(segmentation_mask == class_keys)\n",
    "            #class_keys = class_keys.item\n",
    "            #class_rep = class_rep.item\n",
    "\n",
    "            class_counts[class_keys.item()] = class_rep.item()\n",
    "            #print(class_counts)\n",
    "            #time.sleep(1)\n",
    "            \n",
    "    #print('the label is -->',labels.item())\n",
    "    # Trim the class_counts tensor to the actual number of classes found\n",
    "    #class_counts = class_counts[:num_classes]\n",
    "    \n",
    "    # Print the class counts vector\n",
    "    '''print(\"Class Counts:\")\n",
    "    print(class_counts.values())\n",
    "   \n",
    "    \n",
    "    print('feature_array len-->',len(feature_array),'target_labels len-->',len(target_labels))\n",
    "    print('feature_array -->',(feature_array),'target_labels -->',(target_labels))\n",
    "    #print(labels)\n",
    "    #time.sleep(0)'''\n",
    "    \n",
    "    \n",
    "    feature_array.append(list(class_counts.values()))\n",
    "    target_labels.append(labels.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec1faa",
   "metadata": {},
   "source": [
    "### Dataset and dataloader for the scene classification dataset (Anthroprotect dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265d734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import tifffile as tiff\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.images = self.data['file']\n",
    "        self.labels = self.data['label']\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index]  # Assuming the image names are in the first column of the CSV file\n",
    "        image_label = self.labels[index]\n",
    "        # Construct the complete image path by joining the folder path and image name\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "\n",
    "        # Open the image using PIL\n",
    "        image = tiff.imread(image_path)\n",
    "        \n",
    "        #choos ethe number of channels you need\n",
    "        image= image[:,:,:3]\n",
    "        #####\n",
    "        \n",
    "        image = image.astype('uint8')\n",
    "        #image = image.astype('int')\n",
    "        # Apply transformations, if provided\n",
    "        if self.transform:\n",
    "            #pil_image = Image.fromarray(image)\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        return image, image_label\n",
    "    \n",
    "    \n",
    "csv_file =  'infos.csv'\n",
    "image_folder = '~/working_folder/data/anthroprotect/tiles/s2'\n",
    "transform = transforms.ToTensor()  # Example transformation, you can add more\n",
    "\n",
    "dataset = CustomDataset(csv_file, image_folder, transform=transform) # Replace with your actual dataset instantiation\n",
    "\n",
    "# Define the ratio of the dataset to be used for testing\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Calculate the number of samples for testing\n",
    "test_size = int(test_ratio * len(dataset))\n",
    "\n",
    "# Calculate the number of samples for training\n",
    "train_size = len(dataset) - test_size\n",
    "\n",
    "# Split the dataset into training and testing datasets\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader instances for training and testing datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349b39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),  # Resize the image to (224, 224)\n",
    "    transforms.ToTensor()  # Convert the PIL Image to tensor\n",
    "])\n",
    "scene_dataset= CustomDataset(csv_file, '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2',transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf52b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23919"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128  # Define the batch size\n",
    "data_loader = DataLoader(scene_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a08b7",
   "metadata": {},
   "source": [
    "### a classifier for scence classification, with a method to extract a the embeddings from the bottelneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1356f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeepClassifier, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(256 * 16 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(10,1),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        flattened = torch.flatten(features, start_dim=1)\n",
    "        embedding = self.embedding(flattened)\n",
    "        output = self.classifier(embedding)\n",
    "        \n",
    "        return output, embedding\n",
    "\n",
    "# Example usage\n",
    "input_channels = 3\n",
    "image_size = 256\n",
    "num_classes = 1\n",
    "model = DeepClassifier(num_classes)\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Training the model\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22652b22",
   "metadata": {},
   "source": [
    "### Training of the scene classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d964b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|█▎                       | 1/20 [01:00<19:03, 60.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|██▌                      | 2/20 [02:01<18:11, 60.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|███▊                     | 3/20 [03:00<17:04, 60.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|█████                    | 4/20 [04:01<16:05, 60.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██████▎                  | 5/20 [05:01<15:05, 60.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███████▌                 | 6/20 [06:01<14:04, 60.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|████████▊                | 7/20 [07:02<13:05, 60.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|██████████               | 8/20 [08:03<12:05, 60.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|███████████▎             | 9/20 [09:03<11:03, 60.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|████████████            | 10/20 [10:05<10:09, 60.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████████████▏          | 11/20 [11:06<09:09, 61.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████████████▍         | 12/20 [12:09<08:13, 61.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|███████████████▌        | 13/20 [13:14<07:17, 62.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|████████████████▊       | 14/20 [14:17<06:16, 62.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  75%|██████████████████      | 15/20 [15:20<05:13, 62.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|███████████████████▏    | 16/20 [16:25<04:13, 63.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  85%|████████████████████▍   | 17/20 [17:29<03:10, 63.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████████████████▌  | 18/20 [18:33<02:07, 63.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|██████████████████████▊ | 19/20 [19:38<01:03, 63.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████████| 20/20 [20:42<00:00, 62.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 20\n",
    "# Assuming you have your training data in a PyTorch DataLoader called \"train_loader\"\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
    "    # Variables to keep track of accuracy and total samples\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for data in data_loader:\n",
    "        inputs, labels = data  \n",
    "        labels = labels.float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, encoded_data = model(inputs.float())\n",
    "        \n",
    "        \n",
    "        \n",
    "        labels = labels.view(-1, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        total_samples += inputs.size(0)\n",
    "        \n",
    "    mse = total_loss / total_samples\n",
    "    print(f\"Training MSE: {mse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ca480",
   "metadata": {},
   "source": [
    "### Logistic regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transparent model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#add the mbedding here\n",
    "\n",
    "X = feature_array. append(embedding)\n",
    "\n",
    "\n",
    "##get the target values from the classifier\n",
    "target = target_labels\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Scale the input features\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model to the data\n",
    "logreg.fit(X_scaled, target)\n",
    "\n",
    "# Print the coefficients of the trained model\n",
    "coefficients = logreg.coef_[0]\n",
    "for feature, coefficient in zip(clcc.keys(), coefficients):\n",
    "    print(f\"{feature}: {coefficient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a86779",
   "metadata": {},
   "source": [
    "### visulaization the coefficients of the LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the coefficients of the trained transparent model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a trained logistic regression model named `logreg`\n",
    "coefficients = coefficients\n",
    "feature_names = list(clcc.values())  # Assuming `input_data` contains the feature names\n",
    "\n",
    "# Sort the coefficients and feature names in descending order of absolute magnitude\n",
    "sorted_indices = np.argsort(np.abs(coefficients))[::-1]\n",
    "sorted_coefficients = coefficients[sorted_indices]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 16))\n",
    "plt.barh(range(len(sorted_coefficients)), sorted_coefficients, align='center')\n",
    "plt.yticks(range(len(sorted_coefficients)), sorted_feature_names)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Importance of Coefficients in Logistic Regression Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97110bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a trained logistic regression model named `logreg`\n",
    "\n",
    "  # Assuming `input_data` contains the feature names\n",
    "\n",
    "# Sort the coefficients and feature names in descending order of absolute magnitude\n",
    "sorted_indices = np.argsort(np.abs(coefficients))[::-1]\n",
    "sorted_coefficients = coefficients[sorted_indices]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# Separate positive and negative coefficients\n",
    "positive_indices = np.where(sorted_coefficients >= 0)[0]\n",
    "negative_indices = np.where(sorted_coefficients < 0)[0]\n",
    "\n",
    "positive_coefficients = sorted_coefficients[positive_indices]\n",
    "positive_feature_names = [sorted_feature_names[i] for i in positive_indices]\n",
    "\n",
    "negative_coefficients = sorted_coefficients[negative_indices]\n",
    "negative_feature_names = [sorted_feature_names[i] for i in negative_indices]\n",
    "\n",
    "# Create separate plots for positive and negative coefficients using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Positive coefficients plot\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=positive_coefficients, y=positive_feature_names)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Positive Coefficients')#\n",
    "\n",
    "# Negative coefficients plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=negative_coefficients, y=negative_feature_names)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Negative Coefficients')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b573d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "landcover_classes = [111,\n",
    " 112,\n",
    " 121,\n",
    " 122,\n",
    " 123,\n",
    " 124,\n",
    " 131,\n",
    " 132,\n",
    " 133,\n",
    " 141,\n",
    " 142,\n",
    " 211,\n",
    " 212,\n",
    " 213,\n",
    " 221,\n",
    " 223,\n",
    " 231,\n",
    " 241,\n",
    " 242,\n",
    " 243,\n",
    " 244,\n",
    " 311,\n",
    " 312,\n",
    " 313,\n",
    " 321,\n",
    " 322,\n",
    " 323,\n",
    " 324,\n",
    " 331,\n",
    " 332,\n",
    " 333,\n",
    " 334,\n",
    " 335,\n",
    " 411,\n",
    " 412,\n",
    " 421,\n",
    " 422,\n",
    " 423,\n",
    " 511,\n",
    " 512,\n",
    " 521,\n",
    " 522,\n",
    " 523]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135f312",
   "metadata": {},
   "source": [
    "### turning the ground truth segmenation masks into one-hot encoded L* W * C matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27494ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def onehot_code(csv_file, images_folder, onehot_folder):\n",
    "    #image_path = \"onehotcoded_masks\"\n",
    "    image_path=onehot_folder\n",
    "    csv_file = pd.read_csv(csv_file)\n",
    "    masks_names= csv_file['file']\n",
    "    masks_paths = [os.path.join(images_folder, name) for name in masks_names ] # a list of the masks paths\n",
    "    print(masks_paths[:5])\n",
    "    for n,mask  in enumerate(tqdm(masks_paths)):\n",
    "        tiff_mask = tiff.imread(mask)\n",
    "        target= tiff_mask[:,:,0] # chosse the corine channel\n",
    "        #target = target.astype('uint8')\n",
    "        \n",
    "\n",
    "\n",
    "        # Create an empty tensor with the appropriate size\n",
    "        one_hot = np.zeros((target.shape[0], target.shape[1], len(landcover_classes)))\n",
    "        \n",
    "\n",
    "        # Iterate over the unique class codes and assign 1 to the corresponding channel\n",
    "        for i, code in enumerate(landcover_classes):\n",
    "\n",
    "            one_hot[:, :, i] = (target == code).astype(int)\n",
    "            transposed_one_hot = np.transpose(one_hot, (2,0 , 1))\n",
    "            \n",
    "        im = tiff.imsave(f\"{onehot_folder}/{masks_names[n]}\", transposed_one_hot)\n",
    "      \n",
    "                    \n",
    "\n",
    "                \n",
    "   \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e874d0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs/anthropo_10.20696-59.31534_0.tif', '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs/anthropo_10.20696-59.31534_1.tif', '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs/anthropo_10.20696-59.31534_10.tif', '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs/anthropo_10.20696-59.31534_11.tif', '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs/anthropo_10.20696-59.31534_12.tif']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23919/23919 [15:46<00:00, 25.27it/s]\n"
     ]
    }
   ],
   "source": [
    "onehot_code('infos.csv','/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs',\"onehotmasked_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59fb88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original landcover_classes\n",
    "landcover_classes = [111, 112, 121, 122, 123, 124, 131, 132, 133, 141, 142,\n",
    "                     211, 212, 213, 221, 223, 231, 241, 242, 243, 244,\n",
    "                     311, 312, 313, 321, 322, 323, 324, 331, 332, 333, 334, 335,\n",
    "                     411, 412, 421, 422, 423, 511, 512, 521, 522, 523]\n",
    "\n",
    "# Create a mapping dictionary to map the original classes to the new ones (0 to 43)\n",
    "mapping_dict = {cls: idx for idx, cls in enumerate(landcover_classes)}\n",
    "os.makedirs(\"new_masks\", exist_ok=True)\n",
    "\n",
    "def class_code(csv_file, images_folder):\n",
    "    #image_path = \"onehotcoded_masks\"\n",
    "    \n",
    "    csv_file = pd.read_csv(csv_file)\n",
    "    masks_names= csv_file['file']\n",
    "    masks_paths = [os.path.join(images_folder, name) for name in masks_names ] \n",
    "# Assuming you have a list of segmentation masks as NumPy arrays in 'dataset'\n",
    "    for i, mask in enumerate(tqdm(masks_paths)):\n",
    "        tiff_mask = tiff.imread(mask)\n",
    "        target= tiff_mask[:,:,0]\n",
    "        # Remap the pixel values in the segmentation mask using the mapping dictionary\n",
    "        new_mask = np.array([mapping_dict.get(cls, 0) for cls in target], dtype=int)\n",
    "        #print(np.unique(new_mask))\n",
    "        \n",
    "        \n",
    "# Save the new dataset to a file (e.g., a .npy file)\n",
    "    np.save(os.path.join(\"new_masks\", f\"{mask}\"), new_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b94662b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23919/23919 [02:49<00:00, 141.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Original landcover_classes\n",
    "landcover_classes = [111, 112, 121, 122, 123, 124, 131, 132, 133, 141, 142,\n",
    "                     211, 212, 213, 221, 223, 231, 241, 242, 243, 244,\n",
    "                     311, 312, 313, 321, 322, 323, 324, 331, 332, 333, 334, 335,\n",
    "                     411, 412, 421, 422, 423, 511, 512, 521, 522, 523]\n",
    "\n",
    "# Create a mapping dictionary to map the original classes to the new ones (0 to 43)\n",
    "mapping_dict = {cls: idx for idx, cls in enumerate(landcover_classes)}\n",
    "os.makedirs(\"new_masks\", exist_ok=True)\n",
    "\n",
    "def remap_class(cls):\n",
    "    return mapping_dict.get(cls, 0)\n",
    "\n",
    "def class_code(csv_file, images_folder):\n",
    "    csv_file = pd.read_csv(csv_file)\n",
    "    masks_names = csv_file['file']\n",
    "    masks_paths = [os.path.join(images_folder, name) for name in masks_names] \n",
    "\n",
    "    for i, mask in enumerate(tqdm(masks_paths)):\n",
    "        tiff_mask = tiff.imread(mask)\n",
    "        target = tiff_mask[:, :, 0]\n",
    "        \n",
    "        # Remap the pixel values in the segmentation mask using the mapping dictionary\n",
    "        new_mask = np.vectorize(remap_class, otypes=[int])(target)\n",
    "        \n",
    "        # Save the new mask to the 'new_masks' folder\n",
    "        #filename = os.path.splitext(os.path.basename(mask))[0]  # Get the filename without extension\n",
    "        im = tiff.imsave(f\"new_masks/{masks_names[i]}\", new_mask)\n",
    "\n",
    "# Call the function with the input csv_file and images_folder\n",
    "class_code('infos.csv', '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/lcs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "674b0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace 'folder_path' with the path of the folder containing your images\n",
    "folder_path = '/home/ahmedemam576/greybox/corine_images/new_masks/'\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter only the files without the '.tif' extension (assuming they are the images)\n",
    "image_files = [file for file in file_list if not file.endswith('.tif')]\n",
    "\n",
    "# Rename each image file by adding the '.tif' extension\n",
    "for image_file in image_files:\n",
    "    old_path = os.path.join(folder_path, image_file)\n",
    "    new_path = os.path.join(folder_path, f\"{os.path.splitext(image_file)[0]}.tif\")\n",
    "    os.rename(old_path, new_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f13af39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPPklEQVR4nO3dXYhk6VnA8eetj+6e6Zme6ZnJzs7uzppM1kRJ2ESNGBExCBITFXMRDAoiIoh66ceFF155I6h4p15ILgQR/AARBM2dQVcTopAlINmsO5OdnV0zm52Pnpn+rHOOF5EnkezMVJ3uqlPV9ftdztRb54Gmzr/qnO63StM0TQBARPS6HgCA+SEKACRRACCJAgBJFABIogBAEgUAkigAkAbjPvBj7/r1R/7/9U9ejhd/448nHuCgqeKjv/DLceIrNyde24Xm9t2otra6HoMl1N/cjLJxavKFpUR96uTRD0R7vYgHVzai7peZHvaFv/7Nxz7GJwUAkigAkEQBgDT2PYXHufi57Xj+D35t4nWliXj6jbeOaozx1E3Ut25HtNgLsNnfn8JAAPPhyKKwcv2tuHx9xif3Q6i3t1tFAeA4c/kIgCQKACRRACCJAgBJFABIogBAEgUAkigAkI7sj9cAmFxpIlZvHUSZkz+mFQVYFFUVMaomX1dKlINRu2P2e9H0XFCYht5BE9FElCpieGs7ou56om8QBVgQ1dZWxIy/y2Nw+ZlozrT4DgcerY448dV7XU/xtrwFACDNxyeF/YOZbk7XzMm1O4B5030U6iZGX3szom5xrRSAI+XyEQBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABI3W9zAYylv7kZZWO2O5Y2a6szPR7dEwVYEGVtNerT612PwTHn8hEAqftPCr0SgycutFpaP9iO+t58flEFwCLqPgoREasrrZaVvf0jHgRgubl8BEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAECaj79oBiIior+xETF8yMvSjqXMgCjAHNn70HfG3ubbvyxPfH0/Bm/tzHgils1CR6Gsn4z+yjCqm29GNE3X48Dj9fpR/9Dz0QzK2/73wUZ/xgPB/7fQUYh+L0ppt5kedKH0SuydG0b9kChA19xoBiCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIi733ESyRwe2diKvXZ3rM3hMXol4/MdNj0i1RgBZ66+vRO3tm8oWDfkTLvfDKQRXVgwftFrfUG23O9Hh0TxSghfLMpbj7/vNdjwFHzj0FAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAbbxxMza+eDNK3fUgcLREAVqotraivnY9St10PQocKVEAIIkCAEkUYEHUp9aif/GJrsfgmBMFaKvM9uVz/9mTsf2h74goLbdZhTHYJZWlN7jyzth+zzsmXteUiHrgBM3xIgosvWY4iIN1H5ohwuUj4FGaJkrj126XiSgADzW68Xo0117regxmSBSAh2uaiKrqegpmSBQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASDbE46F6a2tRnrk024PeexDV127O9phAGj8K434Xba/lVsJtv+vWZl1TU85sxNYHZvulLidf340iCtCZsaMweu3G4x9UejG4dDFi0J94kGZ7O6pbtyde943FwgBwFMb/pDDOibc5xMZZdePkDtAxN5oBSKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQLJLKvOlRERv8r2zDqXtJo5wDIkCc2Xn4lr0Pv59Mz1m41UA6chfDvXWvSj9ya9KNbt7Rz0KC6jpRVSr3rlDV44+CvfuHfVTAjAjbjQDkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACT7Q3JsnHrlfpSqmnjdaPNE7FxYmcJEsHhEgWOhNBHly1ej3t6eeO3wvc/FzoXzU5gKFo/LRwAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAyd5HcByVEv33vDti0D/8c42qKF+/3Wppc+Z0NEOnmUXip8XD1U3095uupxhPExF13fUUM9FbXY2mefTPpfT7cf+7zkU9KIc+3sq9KoafebnV2sGJNVFYMH5aPFT15pux9g+3uh5jbHU9+bbZi2a01ov7H/vAWI9tXBymBVFoo9ePwaWLrZbWt++02t65M0twol00TvZMkyi0UHolmrOnoymTfzQv2zsRixQFYKl4zwFAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBA8hfNsCR6VROlxa4lvYNDbDRY11GqFgctJZqe96xdEAVYEqdfvBmja9dneszRq6+1Wtc/fTri2aeOeBrGIQqwLOp69hscPmaL74cvW5At24+hsaNQhivTnGOhFPvDA8fU+FF475VpzrFwvI8BjiN3cgBIogBAEgUAkigAkEQBgCQKACRRACCJwoyVjVPR39zsegyAtyUKM1afXo+4IArAfBIFAJJNfICp6V84H2V1dfKFw0EcYsNuDkEUWHrlYBSD7XanoNHJ2X7YLk1Ef6fl6fJgdLTDjKGcPhX1+omZH5f2RIGlN3rlWqy+cm3idWW4Etsf/2DUg3L0Qz1Ef6+O1X/8QqstqWefBBaRewrQVuMCB8ePKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAZO8jOITBTh2jE72Z7n90ZEqJMhhO/RgsFlGAlprRKIaf+UKsfM/74v6VU12PM7HeqVNRnn1qqseoRWHhiAIcUu/qa3HmzRlFoapi1GKH1Ij/+26D9ZPf8g99J22+jSjAIVV37kbcudv1GI9VTpz4xtfBwiO40QxAEgUAkigAkEQBlkhpeZOa5SEKsCRGN96I5tXXux6DOScKsCzqKuLgoOspmHOiAEASBQCSKACQRAGAJAoAJHsfAdF7sBPNW7cf/aCLF6JZXZnNQHRGFGCJNE0Tvd29b//3+9tRbW09cu3gwrnwp2/HnyjAEmn29qJ6+WrXYzDHlj4Kpaqi+eqNmR6zrqqZHg9gXEsfhaibqLe3u54CYC747SMAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACQb4vVK9M+eiagn3ym+3t6OZjSawlAA3Vj6KDT9fsQzl1qt7d34WlS3H/NtVQALxOUjAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgjb1LavPlV8Z+0v75zagvbLYaCIDujB+Fg/2xn9R3DAAsJpePAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQDS2HsfAfNjcOnJaE6vz/SYzdDpYhmM/VPunz839pOW9ZPRtBimWe3HK5/ciHrYZnU7/Z0SV/7mTkQ1u2PCoQ0G0awMu56CY2j8XVIvPTH2k7Y9vdZrw/inn/v9eNfwVMtnmNy/7tbxu3/381GqambHBJhX7ikAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACBNZS/cZtiPZrU/8brRekdb89YRpa4nX9e0WAMwx6ZyFv7vn92Iv/+ZP5x4Xb808ezg5BQmerjdZhjlpWtR7exOvri2sypwvEwlCtVaHd+9MtuTe1tV04tmNHKCB4gp3VPob/fi83sHUbm8ArBQphKF5/7yTvz2L/1K3Ki2p/H0AEzJdH77qGqiv1f5hkuABeNXUgFIogBAEgUA0tJH4Yn+/bj9qe+NwZV3dj0KQOeWPgofXF2Nz/3en8TrP/5U16MAdG7powDAN4kCAEkUAEiiAEAae0O8L//WZBvc9Yd1nOtPvn12RMQnvvLR+NLnr0y8rtoYxUs/+acxLO2OC7Dsxo7CKz/26RZPf6LFmoj/+uyVePfv/NvE6/rvfS72fuJAFABacvkIgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCNvfcRMD+au1tR9vcnXlcGg6g3N6YwEcfF8YpCVcULu6djvbc38dLBbjOFgWA6qq2tiK2tidf11tYiRIFHOFZRqF6+Gn/0/PdHlDLx2nM7n5/CRACL5VhFISKi3t7uegSAheVGMwBJFABIogBAEgUAkigAkEQBgCQKsESa0Sh6d+5FqaquR2FOiQIskWY0itFrN6LsHXQ9CnNKFABIogBAOnbbXMDCKSWieH/GfBAF6Njgmaej2Vif6THrFptGshxEAbrWK9E4STMnfGYFIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAECyIR7T1evHzV/9gRjNdhPQ1i79y4MoL3yx6zGgM1OLwkFTxZ9vPR0HTX/itau3F2vHyLK62vUI01dV0YxGEy8rvRJbH96JM2e2pzDU0bt3fTM2Xuh6CujO1KLw6mgn/vYjz0d1882J115qFudV2Vtbi3junV2PMXW9u/djdP21rscApmy6l4/qJqJppnoIAI6OG80AJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEhT2RDvI1/6RBz82ZNx+s5/TuPpOYRrnzwX+2fqideV6mz0Di5PfsAScWJ9a/J1QCfGjsIvvvrDYz/pzc8+FZf/6oWwP+r82b28H5vvuNf1GMCcGjsKr394/BPJ5Vic70MA4JvcUwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAaSob4sGj7PzH+Tj7lck35RtXtVJi56fvxrBfTbz2f36kjlvv/8EpTPVwT/57FSev2jSQ+SAKTN3t189EfEsDLr5cx9kX70ztePWplXjwUyWiP/nazafuRjx19DM9ys5L5+Lk1dkeEx5GFJiqUd2L5/7iIAa3d7oeBRiDewoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAECyS+oh1Xt70Xv5Wqu15ZlL0aytHu1AAIcgCofVNFHv7rZaOqjqaI54HIDDcPkIgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACDZ+4i5cvd9Z+ONH60iIuLsi8N48p9vTfwcvZ2DeMenT0XTK0c93kPtn+7F7qfuRCl2s2KxicJh9frR3zjVamkz6B/xMPOnRMRb71uL4fbaWI+/92yJzUu3IyJi79r5dgetmjjx1Xvt1rY0vHAytpsSfVFgwYnCIfVWhhFPPxlNmd270kXS79URH78VB2M+frx0ANPinsIh1bu7Ub90NcruXtejAByaKByB5mA/Su2yAbD4RAGAVJqm8RYXgIjwSQGAbyEKACRRACCJAgBJFABIogBAEgUAkigAkEQBgPS/bXLh2gd0YkIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tiff_mask = '/home/ahmedemam576/greybox/corine_images/new_masks/anthropo_5.52111-59.46264_0'\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "\n",
    "# Load the TIFF image\n",
    "image = tiff.imread(tiff_mask)\n",
    "print(image.shape)\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Optional to hide axis ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e034ce12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages (4.7.0.68)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages (from opencv-python) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a26f3a",
   "metadata": {},
   "source": [
    "### Fine tunning pretrained segmentation model deeplapv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5bfebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      " 46%|████████████████▍                   | 5449/11960 [54:16<1:04:51,  1.67it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8533/3157128924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.models import segmentation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, target_folder,transform_image=None,transform_target=None):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        \n",
    "        #the scene input images and the target segmentation maps both have the same name but in different folder\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.image_paths = self.data['file']\n",
    "        self.image_folder = image_folder\n",
    "        self.target_folder = target_folder\n",
    "        \n",
    "        \n",
    "        self.transform = transform\n",
    "        self.transform_target = transform_target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "  \n",
    "\n",
    "        \n",
    "        # Construct the complete image path by joining the folder path and image name\n",
    "        image_name = self.image_paths[index] \n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "\n",
    "        \n",
    "        # Open the image using PIL\n",
    "        image = tiff.imread(image_path)\n",
    "        #choos ethe number of channels you need\n",
    "        image= image[:,:,:3]\n",
    "\n",
    "        image = image.astype('uint8')\n",
    "        \n",
    "        #\n",
    "        \n",
    "        target_path = os.path.join(self.target_folder, image_name)\n",
    "        target = tiff.imread(target_path)\n",
    "        target = np.transpose(target, (1,2,0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        target = cv2.resize(target, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        #target = target.astype('uint8')\n",
    "        \n",
    "        ## turning the ground truth segmenation mask into a (L,W,CLASSES) one hot-hot encoded matrix\n",
    "        #target = Image.fromarray(target)\n",
    "        #target = target.resize((512,512))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            \n",
    "            image = self.transform(image)\n",
    "            target = self.transform_target(target)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    #-----------------------------------------------------------------------------------#\n",
    "\n",
    "# Define the paths to your training data\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model and set it to training mode\n",
    "\n",
    "model = segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[-1] = torch.nn.Conv2d(256, 43, kernel_size=1)\n",
    "model.train()\n",
    "model.to('cuda')\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the transformation for input images and labels\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    \n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "])\n",
    "\n",
    "#transforms.ToPILImage(),\n",
    "#transform_target =transforms.Compose([transforms.Resize((512, 512)),\n",
    "#                                         transforms.ToTensor()])\n",
    "\n",
    "transform_target = transforms.ToTensor()\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "csv_file =  'infos.csv'\n",
    "image_folder = '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2'\n",
    "target_folder='/home/ahmedemam576/greybox/corine_images/onehotcoded_masks/'\n",
    "    \n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(csv_file, image_folder, target_folder,transform_image=transform, transform_target =transform_target)\n",
    "\n",
    "# Create the data loader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, targets in tqdm(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        images, targets = images.cuda(), targets.cuda()\n",
    "        # Forward pass\n",
    "        outputs = model(images)        \n",
    "        loss = criterion(outputs['out'], targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), 'fine_tuned_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90160903",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609acb8",
   "metadata": {},
   "source": [
    "## loading the weights to segmentation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a34e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  42009728\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import segmentation\n",
    "\n",
    "# Create an instance of the DeepLabv3 model\n",
    "model = segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[-1] = torch.nn.Conv2d(256, 43, kernel_size=1)  # Replace 'num_classes' with your desired number of classes\n",
    "\n",
    "# Load the saved checkpoint\n",
    "checkpoint_path = 'fine_model_ep8_BS_4.pth'  # Replace with the actual path to your saved checkpoint file\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "\n",
    "\n",
    "# Optionally, load other items from the checkpoint if needed\n",
    "# For example, you can load the optimizer state_dict, scheduler state_dict, etc.\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"Number of parameters: \", num_params)\n",
    "\n",
    "\n",
    "# Now you can use the loaded model for inference or further training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11481e",
   "metadata": {},
   "source": [
    "## feed the seen images to the segmentation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc911df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d37aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5372898d",
   "metadata": {},
   "source": [
    "## teach the student from the teacher's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff4f0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "        '''self.mp = nn.MaxPool2d(2)\n",
    "        self.dc = double_conv(in_ch, out_ch)'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        # for padding issues, see \n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5568c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, channel_depth, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, channel_depth)\n",
    "        self.down1 = down(channel_depth, channel_depth*2)\n",
    "        self.down2 = down(channel_depth*2, channel_depth*4)\n",
    "        self.down3 = down(channel_depth*4, channel_depth*8)\n",
    "        self.down4 = down(channel_depth*8, channel_depth*8)\n",
    "        self.up1 = up(channel_depth*16, channel_depth*4)\n",
    "        self.up2 = up(channel_depth*8, channel_depth*2)\n",
    "        self.up3 = up(channel_depth*4, channel_depth)\n",
    "        self.up4 = up(channel_depth*2, channel_depth)\n",
    "        self.outc = outconv(channel_depth, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x\n",
    "        #return x\n",
    "\n",
    "class UNet16(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 16)\n",
    "        self.down1 = down(16, 32)\n",
    "        self.down2 = down(32, 64)\n",
    "        self.down3 = down(64, 128)\n",
    "        self.down4 = down(128, 128)\n",
    "        self.up1 = up(256, 128)\n",
    "        self.up2 = up(128, 64)\n",
    "        self.up3 = up(64, 32)\n",
    "        self.up4 = up(32, 16)\n",
    "        self.outc = outconv(16, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return F.sigmoid(x)\n",
    "        #return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22cdda23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  840491\n"
     ]
    }
   ],
   "source": [
    "student = UNet(channel_depth = 16, n_channels = 3, n_classes = 43)\n",
    "\n",
    "num_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "print(\"Number of parameters: \", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7fc14",
   "metadata": {},
   "source": [
    "##  Dataset for the teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c7fba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.models import segmentation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "\n",
    "\n",
    "#dataloader for the teacher\n",
    "\n",
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, target_folder,transform_image=None,transform_target=None):\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        \n",
    "        #the scene input images and the target segmentation maps both have the same name but in different folder\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.image_paths = self.data['file']\n",
    "        self.image_folder = image_folder\n",
    "        self.target_folder = target_folder\n",
    "        \n",
    "        \n",
    "        self.transform = transform\n",
    "        self.transform_target = transform_target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "  \n",
    "\n",
    "        \n",
    "        # Construct the complete image path by joining the folder path and image name\n",
    "        image_name = self.image_paths[index] \n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "\n",
    "        \n",
    "        # Open the image using PIL\n",
    "        image = tiff.imread(image_path)\n",
    "        #choos ethe number of channels you need\n",
    "        image= image[:,:,:3]\n",
    "\n",
    "        image = image.astype('uint8')\n",
    "        \n",
    "        #\n",
    "        \n",
    "        target_path = os.path.join(self.target_folder, image_name)\n",
    "        target = tiff.imread(target_path)\n",
    "        target = np.transpose(target, (1,2,0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        target = cv2.resize(target, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        #target = target.astype('uint8')\n",
    "        \n",
    "        ## turning the ground truth segmenation mask into a (L,W,CLASSES) one hot-hot encoded matrix\n",
    "        #target = Image.fromarray(target)\n",
    "        #target = target.resize((512,512))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        if self.transform:\n",
    "            \n",
    "            image = self.transform(image)\n",
    "            target = self.transform_target(target)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    #-----------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a489d",
   "metadata": {},
   "source": [
    "## initialize the models and the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your training data\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the model and set it to training mode\n",
    "\n",
    "model = segmentation.deeplabv3_resnet50(pretrained=False)\n",
    "model.classifier[-1] = torch.nn.Conv2d(256, 43, kernel_size=1)\n",
    "model.train()\n",
    "model.to('cuda')\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the transformation for input images and labels\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    \n",
    "    transforms.Resize((512,512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "])\n",
    "\n",
    "#transforms.ToPILImage(),\n",
    "#transform_target =transforms.Compose([transforms.Resize((512, 512)),\n",
    "#                                         transforms.ToTensor()])\n",
    "\n",
    "transform_target = transforms.ToTensor()\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "csv_file =  'infos.csv'\n",
    "image_folder = '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2'\n",
    "target_folder='/home/ahmedemam576/greybox/corine_images/onehotcoded_masks/'\n",
    "    \n",
    "# Create the custom dataset\n",
    "dataset = Train_Dataset(csv_file, image_folder, target_folder,transform_image=transform, transform_target =transform_target)\n",
    "\n",
    "# Create the data loader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "                #############################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a6694",
   "metadata": {},
   "source": [
    "## loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19e9db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#NOTE: all verifications for the size is left\n",
    "\n",
    "T = 5\n",
    "alpha = 0.9\n",
    "\n",
    "\n",
    "def dice_loss(output, gt, smooth = 1):\n",
    "    output = output.clamp(min = 0, max = 1)\n",
    "    intersection = torch.sum(gt*output)\n",
    "    union = torch.sum(gt) + torch.sum(output)\n",
    "    dice = 1 -((2*intersection+smooth) / (union + smooth))\n",
    "    return dice\n",
    "\n",
    "def general_loss(student_output, gt):\n",
    "    #use torch.nn.CrossENtropyLoss()\n",
    "    loss = dice_loss(student_output, gt)\n",
    "    return loss\n",
    "\n",
    "def pixel_wise_loss(student_output, teacher_output):\n",
    "    N,C,W,H = student_output.shape\n",
    "\n",
    "    #what would happen if we use softmax?\n",
    "    pred_T = torch.sigmoid(teacher_output/T)\n",
    "    pred_S = torch.sigmoid(student_output/T).log()\n",
    "\n",
    "    #criterion = torch.nn.KLDivLoss(reduction = 'batchmean')\n",
    "    #KLDloss = - criterion(pred_S, pred_T)\n",
    "    #TODO: map this to KLDL\n",
    "    #KDloss = - sum(p * log (p/q)) ---> refer notes page 15 - 16 \n",
    "    #Pixelwise loss = sum(-p*logq)\n",
    "    #KLDiv = relative entropy\n",
    "    pixelwise_loss = (- pred_T * pred_S)\n",
    "\n",
    "    return  torch.sum(pixelwise_loss) / (W*H)\n",
    "\n",
    "def loss_fn_kd(student_output, teacher_output, gt ):\n",
    "    '''student_output = student_output.round() \n",
    "    student_output[student_output<0] = 0\n",
    "    gt = torch.clamp(gt, min = 0, max = 1)\n",
    "    teacher_output = torch.clamp(teacher_output, min = 0, max = 1)'''\n",
    "\n",
    "    student_output = student_output.clamp(min = 0, max = 1)\n",
    "    teacher_output = teacher_output.clamp(min = 0, max = 1)\n",
    "\n",
    "\n",
    "    student_loss = general_loss(student_output, gt)\n",
    "    kd_loss = pixel_wise_loss(student_output, teacher_output)\n",
    "    #not sure about using T, also check KLD\n",
    "    loss = (student_loss*(1-alpha) + (kd_loss)*(alpha)) # as per structured KD paper\n",
    "    return loss    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e256a9",
   "metadata": {},
   "source": [
    "## fetch the teacher predictions and training loop for the stedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_teacher_outputs(teacher, train_loader):\n",
    "    print('-------Fetch teacher outputs-------')\n",
    "    teacher.eval().cuda() #no stochasticity heremashy \n",
    "    #list of tensors\n",
    "    teacher_outputs = []\n",
    "    with torch.no_grad():\n",
    "        #trainloader gets bs images at a time. why does enumerate(tl) run for all images?\n",
    "        for i, img in enumerate(train_loader):\n",
    "            print(i, 'i')\n",
    "            '''img = img[0, :, :, :, :]\n",
    "            gt = gt[0, :, :, :, :]'''\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "            output = teacher(img)\n",
    "            teacher_outputs.append(output)\n",
    "    return teacher_outputs\n",
    "\n",
    "                 #############################################################################\n",
    "\n",
    "\n",
    "\n",
    "def train_student(student, teacher_outputs, optimizer, train_loader):\n",
    "    print('-------Train student-------')\n",
    "    #called once for each epoch\n",
    "    student.train().cuda()\n",
    "\n",
    "    summ = []\n",
    "    for i, (img, gt) in enumerate(train_loader):\n",
    "        teacher_output = teacher_outputs[i]\n",
    "        if torch.cuda.is_available():\n",
    "            img, gt = img.cuda(), gt.cuda()\n",
    "            teacher_output = teacher_output.cuda()\n",
    "\n",
    "        img, gt = Variable(img), Variable(gt)\n",
    "        teacher_output =  Variable(teacher_output)\n",
    "\n",
    "        output = student(img)\n",
    "\n",
    "        #TODO: loss is wrong\n",
    "        loss = loss_fn_kd(output, teacher_output, gt)    \n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "        if i % summary_steps == 0:\n",
    "            #do i need to move it to CPU?\n",
    "            output = output.detach().cpu()\n",
    "            gt = gt.detach().cpu()\n",
    "            metric = dice_loss(output, gt)\n",
    "            summary = {'metric' : metric.item(), 'loss' : loss.item()}\n",
    "            summ.append(summary)\n",
    "    \n",
    "    #print('Average loss over this epoch: ' + np.mean(loss_avg))\n",
    "    mean_dice_coeff =  np.mean([x['metric'] for x in summ])\n",
    "    mean_loss = np.mean([x['loss'] for x in summ])\n",
    "    print('- Train metrics:\\n' + '\\tMetric:{}\\n\\tLoss:{}'.format(mean_dice_coeff, mean_loss))\n",
    "    #print accuracy and los"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0646cf0",
   "metadata": {},
   "source": [
    "## evaluate the student on the evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd10424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kd(student, val_loader):\n",
    "    print('-------Evaluate student-------')\n",
    "    student.eval().cuda()\n",
    "\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    loss_summ = []\n",
    "    with torch.no_grad():\n",
    "        for i, (img, gt) in enumerate(val_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                img, gt = img.cuda(), gt.cuda()\n",
    "            img, gt = Variable(img), Variable(gt)\n",
    "\n",
    "            output = student(img)\n",
    "            output = output.clamp(min = 0, max = 1)\n",
    "            loss = dice_loss(output, gt)\n",
    "\n",
    "            loss_summ.append(loss.item())\n",
    "\n",
    "    mean_loss = np.mean(loss_summ)\n",
    "    print('- Eval metrics:\\n\\tAverage Dice loss:{}'.format(mean_loss))\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680b831",
   "metadata": {},
   "source": [
    "## complete training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b70b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "min_loss = 100\n",
    "\n",
    "teacher = UNet(channel_depth = 32, n_channels = 3, n_classes=1)\n",
    "student = UNet(channel_depth = 16, n_channels = 3, n_classes=1)\n",
    "\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "scheduler = StepLR(optimizer, step_size = 100, gamma = 0.2)\n",
    "\n",
    "#load teacher and student model\n",
    "teacher.load_state_dict(torch.load(teacher_weights))\n",
    "#student.load_state_dict(torch.load(student_weights))\n",
    "\n",
    "#NV: add val folder\n",
    "train_list = glob.glob('/home/nirvi/Internship_2020/Carvana dataset/train/train1/*jpg')\n",
    "val_list = glob.glob('/home/nirvi/Internship_2020/Carvana dataset/val/val1/*jpg')\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "#2 tensors -> img_list and gt_list. for batch_size = 1 --> img: (1, 3, 320, 320); gt: (1, 1, 320, 320)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset.listDataset(train_list,\n",
    "    shuffle = False,\n",
    "    transform = tf,\n",
    "    ),\n",
    "    batch_size = 1\n",
    ")\n",
    "\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset.listDataset(val_list,\n",
    "    shuffle = False,\n",
    "    transform = tf,\n",
    "    ),\n",
    "    batch_size = 1\n",
    ")\n",
    "\n",
    "#train_and_evaluate_kd:\n",
    "#get teacher outputs as list of tensors\n",
    "teacher_outputs = fetch_teacher_outputs(teacher, train_loader)\n",
    "print(len(teacher_outputs))\n",
    "for epoch in range(num_of_epochs):\n",
    "    #train the student\n",
    "    print(' --- student training: epoch {}'.format(epoch+1))\n",
    "    train_student(student, teacher_outputs, optimizer, train_loader)\n",
    "\n",
    "    #evaluate for one epoch on validation set\n",
    "    val = evaluate_kd(student, val_loader)\n",
    "    if(val < min_loss):\n",
    "        min_loss = val\n",
    "        #TODO: make min as the val loss of teacher\n",
    "        print('New best!!')\n",
    "\n",
    "\n",
    "    #if val_metric is best, add checkpoint\n",
    "\n",
    "    torch.save(student.state_dict(), 'checkpoints/0.9/16/CP{}.pth'.format(epoch+1))\n",
    "    print(\"Checkpoint {} saved!\".format(epoch+1))\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703a5a3",
   "metadata": {},
   "source": [
    "## Function to produce a list of segmentation maps, each coming from a stochastic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34ce2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout(model, x, len_samples):\n",
    "    model.train()\n",
    "    model = model.to('cuda')\n",
    "    x = x.to('cuda')\n",
    "    \n",
    "    for i in range(len_samples):\n",
    "        output = model(x)\n",
    "        output = output['out']\n",
    "        print('output shape----->',output.shape)\n",
    "        \n",
    "        \n",
    "    return output\n",
    "        \n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da8f123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npretrained_model = model\\ninput_data = dataset\\n\\n\\nmodel = model.to('cuda')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.models import segmentation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "            def __init__(self, csv_file, image_folder, target_folder,transform_image=None):\n",
    "                \n",
    "                self.data = pd.read_csv(csv_file)\n",
    "                \n",
    "                \n",
    "                #the scene input images and the target segmentation maps both have the same name but in different folder\n",
    "                \n",
    "                \n",
    "                \n",
    "                self.image_paths = self.data['file']\n",
    "                #self.image_paths = self.image_paths[:10] for testing and debugging\n",
    "                self.image_folder = image_folder\n",
    "                self.target_folder = target_folder\n",
    "                \n",
    "                \n",
    "                self.transform = transform\n",
    " \n",
    "\n",
    "            def __getitem__(self, index):\n",
    "        \n",
    "\n",
    "                \n",
    "                # Construct the complete image path by joining the folder path and image name\n",
    "                image_name = self.image_paths[index] \n",
    "                image_path = os.path.join(self.image_folder, image_name)\n",
    "\n",
    "                \n",
    "                # Open the image using PIL\n",
    "                image = tiff.imread(image_path)\n",
    "                #choos ethe number of channels you need\n",
    "                image= image[:,:,:3]\n",
    "\n",
    "                image = image.astype('uint8')\n",
    "                \n",
    "                #\n",
    "                \n",
    "                target_path = os.path.join(self.target_folder, image_name)\n",
    "                target = tiff.imread(target_path)\n",
    "                target = np.transpose(target, (1,2,0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                target = cv2.resize(target, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                #target = target.astype('uint8')\n",
    "                \n",
    "                ## turning the ground truth segmenation mask into a (L,W,CLASSES) one hot-hot encoded matrix\n",
    "                #target = Image.fromarray(target)\n",
    "                #target = target.resize((512,512))\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                if self.transform:\n",
    "                    \n",
    "                    image = self.transform(image)\n",
    "  \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "                return image, target\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.image_paths)\n",
    "\n",
    "'''\n",
    "model = segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[-1] = torch.nn.Conv2d(256, 43, kernel_size=1)\n",
    "checkpoint_path = 'fine_model_ep8_BS_4.pth'  # Replace with the actual path to your saved checkpoint file\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path))'''\n",
    "\n",
    "\n",
    "\n",
    "csv_file =  'infos.csv'\n",
    "image_folder = '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2'\n",
    "target_folder='/home/ahmedemam576/greybox/corine_images/onehotcoded_masks/'\n",
    "\n",
    "\n",
    "# Create the custom dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "dataset = CustomDataset(csv_file, image_folder, target_folder,transform_image=transform)\n",
    "\n",
    "\n",
    "train_size = int(0.9955 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for the training and test subsets\n",
    "num_samples = 3\n",
    "batch_size=10\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size , shuffle=False,drop_last=True)\n",
    "# Create the data loader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size , shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "pretrained_model = model\n",
    "input_data = dataset\n",
    "\n",
    "\n",
    "model = model.to('cuda')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb562887",
   "metadata": {},
   "source": [
    "### run the model several times using MC droput, and store the results in a tensor of the shape (no.of batches, number of MCruns, channels(classes),width, hight )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f0169e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10236/2780740822.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mall_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m# Loop over the input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Define your Monte Carlo Dropout function\n",
    "\n",
    "def mc_dropout(model, x, num_samples):\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        model.train()\n",
    "        x = x.to('cuda')\n",
    "        #print(x.shape)\n",
    "        #outputs = torch.zeros((num_samples,) + x.shape[1:])\n",
    "        \n",
    "        preds = [(model(x)['out'].cpu().detach()) for i in range(num_samples)]\n",
    "        #pred = model(x)['out'].cpu().detach()\n",
    "        #print('model output is -->',model(x)['out'].size)\n",
    "        #preds.append(pred)\n",
    "        \n",
    "     \n",
    "    return preds\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty tensor to store the outputs\n",
    "\n",
    "\n",
    "# Create an empty tensor with the desired shape\n",
    "\n",
    "all_predictions = []\n",
    "print(len(test_dataloader))\n",
    "# Loop over the input data\n",
    "for imgs , target in tqdm(test_dataloader):\n",
    "    batch_predictions = []\n",
    "    pred_sets = mc_dropout(pretrained_model, imgs, num_samples)\n",
    "    img_predictions = torch.stack(pred_sets, dim=1)\n",
    "    #batch_predictions.append(img_predictions)\n",
    "    #batch_predictions.append(pred_sets)\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "  \n",
    "    \n",
    "    #batch_predictions = torch.stack(batch_predictions, dim=0)\n",
    "    \n",
    "    all_predictions.append(img_predictions)\n",
    "    \n",
    "\n",
    "    \n",
    "monte_carlo_predictions = torch.cat(all_predictions, dim=0)\n",
    "print('mc predcs--->',monte_carlo_predictions.shape)\n",
    "'''# Calculate uncertainty maps\n",
    "variance = torch.var(outputs, dim=1)  # Variance across the Monte Carlo samples\n",
    "uncertainty_maps = 1 - torch.max(F.softmax(variance, dim=1), dim=1)[0]  # Uncertainty maps\n",
    "\n",
    "# Print the shape of the uncertainty maps tensor\n",
    "print(uncertainty_maps.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "202c320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  0,  0,  ..., 19, 19, 19],\n",
      "         [ 0,  0,  0,  ..., 19, 19, 19],\n",
      "         [ 0,  0,  0,  ..., 19, 19, 19],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[19, 19, 19,  ...,  0,  0,  0],\n",
      "         [19, 19, 19,  ...,  0,  0,  0],\n",
      "         [19, 19, 19,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]]], device='cuda:0')\n",
      "pred torch.Size([10, 512, 512])\n",
      "label torch.return_types.max(\n",
      "values=tensor([[19, 19, 19,  ..., 19, 19, 19],\n",
      "        [19, 19, 19,  ..., 19, 19, 19],\n",
      "        [19, 19, 19,  ..., 19, 19, 19],\n",
      "        ...,\n",
      "        [11, 11, 11,  ..., 11, 11, 11],\n",
      "        [11, 11, 11,  ..., 11, 11, 11],\n",
      "        [11, 11, 11,  ..., 11, 11, 11]], device='cuda:0'),\n",
      "indices=tensor([[2, 2, 2,  ..., 0, 0, 0],\n",
      "        [2, 2, 2,  ..., 0, 0, 0],\n",
      "        [2, 2, 2,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [5, 5, 5,  ..., 4, 4, 4],\n",
      "        [5, 5, 5,  ..., 4, 4, 4],\n",
      "        [5, 5, 5,  ..., 4, 4, 4]], device='cuda:0'))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isnan(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8008/1837325962.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mpredicted_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmIOU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mdice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_dice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8008/1837325962.py\u001b[0m in \u001b[0;36mmIOU\u001b[0;34m(pred, label, num_classes)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0miou_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpresent_iou_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0miou\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miou\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miou_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_iou_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_iou_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8008/1837325962.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0miou_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou_now\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpresent_iou_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0miou\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miou\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miou_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_iou_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_iou_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: isnan(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mIOU(pred, label, num_classes=43):\n",
    "    #pred = F.softmax(pred, dim=1)  # Convert raw logits to probabilities\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    label = torch.argmax(label,dim=1)\n",
    "    print(label)\n",
    "    # Convert to class indices\n",
    "    print('pred',pred.shape)\n",
    "    print('label',torch.max(label, dim= 0))\n",
    "    iou_list = []\n",
    "    for sem_class in range(num_classes):\n",
    "        pred_inds = (pred == sem_class)\n",
    "        target_inds = (label == sem_class)\n",
    "\n",
    "        if target_inds.long().sum().item() == 0:\n",
    "            iou_now = float('nan')\n",
    "        else:\n",
    "            intersection_now = (pred_inds & target_inds).long().sum().item()\n",
    "            union_now = (pred_inds | target_inds).long().sum().item()\n",
    "            iou_now = float(intersection_now) / (float(union_now) + 1e-8)\n",
    "\n",
    "        iou_list.append(iou_now)\n",
    "\n",
    "    present_iou_list = [iou for iou in iou_list if not torch.isnan(iou)]\n",
    "\n",
    "    return sum(present_iou_list) / len(present_iou_list)\n",
    "\n",
    "def calculate_dice(y_pred, y_true):\n",
    "    intersection = torch.logical_and(y_pred, y_true).sum(dim=(1, 2, 3)).float()\n",
    "    dice = (2.0 * intersection) / (y_pred.sum(dim=(1, 2, 3)) + y_true.sum(dim=(1, 2, 3)) + 1e-8)\n",
    "    dice = dice.mean()\n",
    "    return dice.item()\n",
    "\n",
    "# Assuming you have loaded your model and validation dataset\n",
    "model = segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[-1] = nn.Sequential(\n",
    "        nn.Conv2d(256, 43, kernel_size=1),\n",
    "        nn.Sigmoid() )\n",
    "model.to('cuda')\n",
    "checkpoint_path = '/home/ahmedemam576/greybox/corine_images/pretrained_CEandexternalSigmoid19.pth'  # Replace with the actual path to your saved checkpoint file\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=batch_size , shuffle=False,drop_last=True)\n",
    "# DataLoader for the validation dataset\n",
    "\n",
    "model.eval()\n",
    "iou_scores = []\n",
    "dice_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in val_dataloader:\n",
    "        images, targets = images.cuda(), targets.cuda()\n",
    "        targets = targets.permute(0,3,1,2)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        predicted_masks = outputs['out']  # Assuming you have a threshold for prediction\n",
    "        predicted_masks = predicted_masks.to('cuda')\n",
    "        targets = targets.to('cuda')\n",
    "        iou = mIOU(predicted_masks, targets)\n",
    "        dice = calculate_dice(predicted_masks, targets)\n",
    "\n",
    "        iou_scores.append(iou)\n",
    "        dice_scores.append(dice)\n",
    "\n",
    "mean_iou = sum(iou_scores) / len(iou_scores)\n",
    "mean_dice = sum(dice_scores) / len(dice_scores)\n",
    "\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "print(f\"Mean Dice Score: {mean_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target, num_classes):\n",
    "    iou_sum = 0\n",
    "    for class_id in range(num_classes):\n",
    "        intersection = np.logical_and(target == class_id, pred == class_id)\n",
    "        union = np.logical_or(target == class_id, pred == class_id)\n",
    "        iou = np.sum(intersection) / np.sum(union)\n",
    "        iou_sum += iou\n",
    "    mean_iou = iou_sum / num_classes\n",
    "    return mean_iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 11906/11906 [18:43<00:00, 10.60it/s]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 43\n",
    "\n",
    "# Initialize an empty list to store the transformed labels\n",
    "y_class_encoded = []\n",
    "\n",
    "# For loop to iterate through the dataset and convert one-hot encoded labels to class encoded\n",
    "for image, one_hot_label in tqdm(train_dataloader):\n",
    "    one_hot_label = one_hot_label.permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Find the class index by finding the position of 1 in the one-hot encoded label\n",
    "    class_index = torch.argmax(one_hot_label, dim=1)\n",
    "    #print(class_index.shape)\n",
    "    # Append the class index to the list of class encoded labels\n",
    "    y_class_encoded.append(class_index)\n",
    "    #print(len(y_class_encoded))\n",
    "# Convert the list of class indices into a tensor\n",
    "# The shape of the tensor will be: (num_images, height, width)\n",
    "stacked_tensor = torch.cat(y_class_encoded, dim=0)\n",
    "print(stacked_tensor.shape)\n",
    "# Save the stacked tensor to a file (e.g., a .pt or .pth file)\n",
    "torch.save(stacked_tensor, \"class_indexed_GT.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbbea2-d063-4bcf-87b5-225f8101490a",
   "metadata": {},
   "source": [
    "### visulaize and evaluate trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24517726-901c-4e63-ad7a-8f8be5a38bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and dataloaders\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.models import segmentation\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, target_folder,transform_image=None,transform_target=None):\n",
    "\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "        #the scene input images and the target segmentation maps both have the same name but in different folder\n",
    "\n",
    "\n",
    "\n",
    "        self.image_paths = self.data['file']\n",
    "        self.image_paths = self.image_paths[:] #for testing and debugging\n",
    "        self.image_folder = image_folder\n",
    "        self.target_folder = target_folder\n",
    "\n",
    "\n",
    "        self.transform = transform\n",
    "        self.transform_target = transform_target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "\n",
    "\n",
    "        # Construct the complete image path by joining the folder path and image name\n",
    "        image_name = self.image_paths[index] \n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "\n",
    "\n",
    "        # Open the image using PIL\n",
    "        image = tiff.imread(image_path)\n",
    "        #choos ethe number of channels you need\n",
    "        image= image[:,:,:3]/10000\n",
    "\n",
    "\n",
    "        target_path = os.path.join(self.target_folder, image_name)\n",
    "        target = tiff.imread(target_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            image = self.transform(image)\n",
    "            target = self.transform_target(target)\n",
    "\n",
    "\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "csv_file =  'infos.csv'\n",
    "image_folder = '/home/ahmedemam576/working_folder/data/anthroprotect/tiles/s2'\n",
    "target_folder='/home/ahmedemam576/greybox/corine_images/new_masks/'\n",
    "\n",
    "# Create the custom dataset\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.ToTensor() \n",
    "])\n",
    "\n",
    "\n",
    "transform_target = transforms.ToTensor()\n",
    "dataset = CustomDataset(csv_file, image_folder, target_folder,transform_image=transform, transform_target =transform_target)\n",
    "batch_size = 40\n",
    "# Create the data loader\n",
    "\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# Calculate the number of samples for each split\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Perform the split\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6757d168-ba69-464d-9d39-bdd986d0114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ahmedemam576/coding_streak_envionment/Anconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHead(\n",
       "    (0): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 43, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (aux_classifier): FCNHead(\n",
       "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model and weights\n",
    "model = segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[-1] = nn.Sequential(\n",
    "nn.Conv2d(256, 43, kernel_size=1))\n",
    "PATH= 'allparm_CE_classindexedGT99.pth'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e95f309f-9803-4d42-a55b-76dc894d81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images batch shape is torch.Size([40, 3, 256, 256]) and targets batch shape is torch.Size([40, 1, 256, 256])\n",
      "image shape is torch.Size([3, 256, 256]) and target shape is torch.Size([1, 256, 256])\n",
      "output shape is torch.Size([40, 43, 256, 256])\n",
      "output shape before softmax is torch.Size([43, 256, 256])\n",
      "output prob shape after sigmoid  is torch.Size([43, 256, 256])\n",
      "binary_predictions shape after threshold is torch.Size([43, 256, 256])\n",
      "predictions shape after argmax is torch.Size([256, 256])\n",
      "preicted classes  tensor([21, 22, 23, 39])\n",
      "ground truth classes tensor([21, 22, 34, 39])\n",
      "pred shape, torch.Size([256, 256])\n",
      "target shape,  torch.Size([256, 256])\n",
      "iou -----> tensor(93.3798)  % \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import tensor\n",
    "from torchmetrics.functional.classification import multiclass_jaccard_index, multilabel_jaccard_index\n",
    "images, targets = next(iter(dataloader))\n",
    "print(f'images batch shape is {images.shape} and targets batch shape is {targets.shape}')\n",
    "image, target = images[0], targets[0]\n",
    "print(f'image shape is {image.shape} and target shape is {target.shape}')\n",
    "\n",
    "\n",
    "input_tensor = images.float()\n",
    "input_tensor\n",
    "model.eval()\n",
    "# Make predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "        outputs = model(input_tensor)['out']\n",
    "    \n",
    "print(f'output shape is {outputs.shape}')\n",
    "output = outputs[0]\n",
    "print(f'output shape before softmax is {output.shape}')# .detach().cpu().numpy()\n",
    "target = target[0]#.detach().cpu().numpy()\n",
    "output_prob = torch.sigmoid(output)\n",
    "\n",
    "print(f'output prob shape after sigmoid  is {output_prob.shape}')\n",
    "\n",
    "#binary_predictions = (output_prob > 0.5).cpu().numpy().astype(int)\n",
    "print(f'binary_predictions shape after threshold is {output_prob.shape}')\n",
    "\n",
    "prediction = torch.argmax(output_prob, dim = 0)\n",
    "print(f'predictions shape after argmax is {prediction.shape}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target = target.long()\n",
    "\n",
    "preds = prediction.long()\n",
    "\n",
    "print('preicted classes ',torch.unique(preds))\n",
    "print('ground truth classes',torch.unique(target))\n",
    "print('pred shape,',preds.shape)\n",
    "print('target shape, ', target.shape)\n",
    "print('iou ----->',multiclass_jaccard_index(preds, target, num_classes= 43, average='weighted')*100, ' % ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the original image and predicted mask\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[1].imshow(target, cmap='jet', vmin=0, vmax=43)  \n",
    "axs[1].set_title('ground truth mask')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[0].imshow(prediction, cmap='jet', vmin=0, vmax=43)\n",
    "axs[0].set_title('predicted mask')\n",
    "axs[0].axis('off')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a1f646",
   "metadata": {},
   "source": [
    "### validate the model performace using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b464eb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images batch shape is torch.Size([40, 3, 256, 256]) and targets batch shape is torch.Size([40, 1, 256, 256])\n",
      "image shape is torch.Size([3, 256, 256]) and target shape is torch.Size([1, 256, 256])\n",
      "output shape is torch.Size([40, 43, 256, 256])\n",
      "output shape before softmax is torch.Size([43, 256, 256])\n",
      "output prob shape after sigmoid  is torch.Size([43, 256, 256])\n",
      "binary_predictions shape after threshold is torch.Size([43, 256, 256])\n",
      "predictions shape after argmax is torch.Size([256, 256])\n",
      "preicted classes  tensor([11, 19, 22, 23, 27])\n",
      "ground truth classes tensor([11, 19, 22, 23])\n",
      "pred shape, torch.Size([256, 256])\n",
      "target shape,  torch.Size([256, 256])\n",
      "iou -----> tensor(76.4240)  % \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import tensor\n",
    "from torchmetrics.functional.classification import multiclass_jaccard_index, multilabel_jaccard_index\n",
    "\n",
    "images, targets = next(iter(test_dataloader))\n",
    "print(f'images batch shape is {images.shape} and targets batch shape is {targets.shape}')\n",
    "image, target = images[0], targets[0]\n",
    "print(f'image shape is {image.shape} and target shape is {target.shape}')\n",
    "###################################\n",
    "\n",
    "input_tensor = images.float()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        outputs = model(input_tensor)['out']\n",
    "    \n",
    "print(f'output shape is {outputs.shape}')\n",
    "output = outputs[0]\n",
    "print(f'output shape before softmax is {output.shape}')# .detach().cpu().numpy()\n",
    "target = target[0]#.detach().cpu().numpy()\n",
    "output_prob = torch.sigmoid(output)\n",
    "\n",
    "print(f'output prob shape after sigmoid  is {output_prob.shape}')\n",
    "\n",
    "#binary_predictions = (output_prob > 0.5).cpu().numpy().astype(int)\n",
    "print(f'binary_predictions shape after threshold is {output_prob.shape}')\n",
    "\n",
    "prediction = torch.argmax(output_prob, dim = 0)\n",
    "print(f'predictions shape after argmax is {prediction.shape}')\n",
    "target = target.long()\n",
    "\n",
    "preds = prediction.long()\n",
    "\n",
    "print('preicted classes ',torch.unique(preds))\n",
    "print('ground truth classes',torch.unique(target))\n",
    "print('pred shape,',preds.shape)\n",
    "print('target shape, ', target.shape)\n",
    "print('iou ----->',multiclass_jaccard_index(preds, target, num_classes= 43, average='weighted')*100, ' % ')\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "# Visualize the original image and predicted mask\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[1].imshow(target, cmap='jet', vmin=0, vmax=43)  \n",
    "axs[1].set_title('ground truth mask')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[0].imshow(prediction, cmap='jet', vmin=0, vmax=43)\n",
    "axs[0].set_title('predicted mask')\n",
    "axs[0].axis('off')\n",
    "##\n",
    "\n",
    "##\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c89b3ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_110030/3279299088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asd' is not defined"
     ]
    }
   ],
   "source": [
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9bae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
